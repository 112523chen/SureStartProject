{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from kaggle\n",
    "od.download(\"https://www.kaggle.com/datasets/hassanamin/textdb3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df for the data\n",
    "df = pd.read_csv(\"textdb3/fake_or_real_news.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create helper functions\n",
    "def convertLabelsToBinary(label):\n",
    "  if label == \"FAKE\":\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "def removeNewLine(text):\n",
    "  return re.sub(r'\\n', '', text)\n",
    "\n",
    "def removeSpecialCharacters(text):\n",
    "  return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def cleanText(text):\n",
    "  text = removeNewLine(text)\n",
    "  text = removeSpecialCharacters(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! create NLP pipeline\n",
    "\n",
    "# title_martix - numpy like object with the shape of (m,1) where m is number of title samples\n",
    "# text_matrix - numpy like obejct with the shape of (m,1) where m is number of text samples\n",
    "# label_matrix - numpy like object with the shape of (m,1) where m is number of label samples\n",
    "# test_size - split of the data that will be used for testing the model\n",
    "# vectorizer_method - tfidf (default), BoW\n",
    "# n_gram_n_min - smallest number of words grouped to together for n_grams (default is 1 word)\n",
    "# n_gram_n_max - largest number of words grouped to together for n_grams (default is 1 word)\n",
    "# min_df - threshold of frequency of words within dataset (default is 0.01 : if word in used in less than 1% of the document it will be removed)\n",
    "\n",
    "def pipeline2(title_matrix, text_matrix, label_matrix, tokenizer, test_size=0.2, vectorizer_method=\"tfidf\", n_gram_n_min=1, n_gram_n_max=1, min_df=0.01 ):\n",
    "\n",
    "  data = pd.concat([title_matrix, text_matrix, label_matrix], axis=1).reset_index()\n",
    "  print(\"Combined Data\")\n",
    "\n",
    "  #updates labels to values of 0 and 1 where 1 is fake\n",
    "  data['label'] = data['label'].apply(convertLabelsToBinary)\n",
    "  print(\"Updated Labels\")\n",
    "\n",
    "  #creates a new column for cleaned up verison of data \n",
    "  data['text'] = data['text'].apply(removeNewLine)\n",
    "  data['text'] = data['text'].apply(removeSpecialCharacters)\n",
    "  print(\"Cleaned Up Text\")\n",
    "\n",
    "  data['words'] =  data['title'] + \" \" + data['text']\n",
    "  print(\"Added Title\")\n",
    " \n",
    "  #splitting up the data\n",
    "  X_train, X_test, y_train, y_test = train_test_split(data['words'], data['label'], test_size=test_size, random_state=42)\n",
    "  print(\"Splited Data\")\n",
    "\n",
    "  #joining the input data for vectorization\n",
    "  X_temp = pd.concat([X_train,X_test])\n",
    "  print(\"Combined Data\")\n",
    "\n",
    "  #preforms vectorization \n",
    "  vectorizer = TfidfVectorizer(ngram_range=(n_gram_n_min,n_gram_n_max), analyzer=\"word\", lowercase=True, tokenizer=tokenizer, stop_words='english', min_df=min_df )\n",
    "  if vectorizer_method == \"BoW\":\n",
    "    vectorizer = CountVectorizer(ngram_range=(n_gram_n_min,n_gram_n_max), analyzer=\"word\", lowercase=True, tokenizer=tokenizer, stop_words='english', min_df=min_df )\n",
    "  vectorizer.fit(X_temp)\n",
    "  X_train = vectorizer.transform(X_train).toarray()\n",
    "  X_test = vectorizer.transform(X_test).toarray()\n",
    "  print(\"Vectorized Data\")\n",
    "\n",
    "  return X_train, X_test, y_train, y_test, vectorizer, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Data and Objects\n",
    "X_train, X_test, y_train, y_test, vectorizer, input_shape = pipeline2(df['title'], df['text'], df['label'], wordpunct_tokenize, \n",
    "vectorizer_method=\"tfidf\", test_size=0.2, n_gram_n_min=1, n_gram_n_max=1, min_df=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Deep Learning Model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=( None, input_shape[1]), activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Deep Learning Model\n",
    "EPOCHS_VALUE = 50\n",
    "history = model.fit(X_train, y_train, epochs=EPOCHS_VALUE, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Data About Model\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(EPOCHS_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Accuracy Across Epochs\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Loss Across Epochs\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting Predictions with New Data\n",
    "y_pred = model.predict(X_test).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Model's Confusion Matrix \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Classification Report on Model\n",
    "print(classification_report(y_test, y_pred, target_names=['REAL','FAKE',]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving all the data and objects\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))\n",
    "pickle.dump(model, open('deepLearningModel.pkl', 'wb'))\n",
    "model.save('deepLearningModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('deepLearningModel.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
